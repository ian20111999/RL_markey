"""ä½¿ç”¨ Optuna æœå°‹ SAC åšå¸‚ agent çš„è¶…åƒæ•¸çµ„åˆã€‚
é€éè‡ªå‹•åŒ– tuning æ¸›å°‘æ‰‹å‹•èª¿æ•´ learning rate / net_arch çš„æ™‚é–“ã€‚
"""
from __future__ import annotations

import argparse
import datetime
import json
from functools import partial
from pathlib import Path
from typing import Any, Callable, Dict, List

import numpy as np
import optuna
from optuna.exceptions import TrialPruned
from stable_baselines3 import SAC
from stable_baselines3.common.monitor import Monitor
from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv, VecEnv
from stable_baselines3.common.callbacks import BaseCallback

from envs.historical_market_making_env import HistoricalMarketMakingEnv
from utils.config import build_env_kwargs, load_config

ROOT = Path(__file__).resolve().parent
DATA_DIR = ROOT / "data"
MODELS_DIR = ROOT / "models"
DEFAULT_CSV = DATA_DIR / "btc_usdt_1m_2023.csv"
BEST_PARAMS_PATH = MODELS_DIR / "best_sac_params.json"

NetArch = List[int]
NET_ARCH_LIBRARY: Dict[str, NetArch] = {
    "64x2": [64, 64],
    "128x2": [128, 128],
    "256x2": [256, 256],
}


def env_section_from_args(args: argparse.Namespace) -> Dict[str, Any]:
    csv_path = Path(args.csv_path)
    if not csv_path.is_absolute():
        csv_path = ROOT / csv_path
    return {
        "csv_path": str(csv_path),
        "episode_length": int(args.episode_length),
        "fee_rate": float(args.fee_rate),
        "lambda_inv": float(args.lambda_inv),
        "lambda_turnover": float(args.lambda_turnover),
        "max_inventory": float(args.max_inventory),
        "base_spread": float(args.base_spread),
        "alpha": float(args.alpha),
        "beta": float(args.beta),
        "random_start": bool(args.random_start),
    }


def apply_config_overrides(args: argparse.Namespace) -> None:
    if not args.config:
        return
    cfg = load_config(args.config)
    env_cfg = cfg.env
    for key in [
        "csv_path",
        "episode_length",
        "fee_rate",
        "lambda_inv",
        "lambda_turnover",
        "max_inventory",
        "base_spread",
        "alpha",
        "beta",
        "random_start",
    ]:
        if key in env_cfg:
            value = env_cfg[key]
            if key == "csv_path":
                value = Path(value)
            setattr(args, key, value)
    train_cfg = cfg.train
    mapping = {
        "total_timesteps": "train_timesteps",
        "buffer_size": "buffer_size",
        "train_freq": "train_freq",
        "gradient_steps": "gradient_steps",
    }
    for src, dest in mapping.items():
        if src in train_cfg:
            setattr(args, dest, train_cfg[src])
def make_env(env_kwargs: Dict[str, Any], seed: int | None = None, random_start: bool = True) -> Callable[[], HistoricalMarketMakingEnv]:
    """å°è£ç’°å¢ƒå»ºç«‹å‡½å¼ï¼Œæ–¹ä¾¿ DummyVecEnv å‘¼å«ã€‚"""

    def _init() -> HistoricalMarketMakingEnv:
        local_kwargs = dict(env_kwargs)
        local_kwargs["random_start"] = random_start
        local_kwargs["seed"] = seed
        env = HistoricalMarketMakingEnv(**local_kwargs)
        return Monitor(env)

    return _init


def evaluate_model(
    model: SAC,
    eval_env: VecEnv,
    n_episodes: int,
    metric: str,
) -> float:
    """
    åœ¨ç¨ç«‹é©—è­‰ç’°å¢ƒä¸Šå›æ¸¬ï¼Œå›å‚³å¹³å‡ç¸¾æ•ˆã€‚
    å„ªåŒ–ï¼šä½¿ç”¨å‚³å…¥çš„ VecEnvï¼Œæ”¯æ´å¹³è¡Œè©•ä¼°ï¼Œä¸”ä¸éœ€é‡è¤‡å»ºç«‹ç’°å¢ƒã€‚
    """
    obs = eval_env.reset()
    n_envs = eval_env.num_envs
    dones = np.array([False] * n_envs)
    episode_rewards = np.zeros(n_envs)
    final_values = np.zeros(n_envs)
    
    # å‡è¨­ n_episodes == n_envsï¼Œä¸€æ¬¡è·‘å®Œ
    while not all(dones):
        action, _ = model.predict(obs, deterministic=True)
        obs, rewards, dones_step, infos = eval_env.step(action)
        
        for i in range(n_envs):
            if not dones[i]:
                episode_rewards[i] += rewards[i]
                if dones_step[i]:
                    dones[i] = True
                    if metric == "portfolio":
                        final_values[i] = infos[i].get("portfolio_value", 0.0)
                    else:
                        final_values[i] = episode_rewards[i]
    
    return float(np.mean(final_values))


def suggest_hyperparams(trial: optuna.Trial) -> Dict[str, object]:
    """å®šç¾© Optuna æœå°‹ç©ºé–“ã€‚"""

    net_arch_label = trial.suggest_categorical("net_arch", list(NET_ARCH_LIBRARY.keys()))
    net_arch_choice = NET_ARCH_LIBRARY[net_arch_label]
    params: Dict[str, object] = {
        "learning_rate": trial.suggest_float("learning_rate", 1e-5, 3e-3, log=True),
        "gamma": trial.suggest_float("gamma", 0.98, 0.999),
        "batch_size": trial.suggest_categorical("batch_size", [64, 128, 256]),
        "tau": trial.suggest_float("tau", 0.01, 0.1),
        "policy_kwargs": {"net_arch": list(net_arch_choice)},
    }
    return params


def decode_net_arch(value: object) -> NetArch | None:
    if isinstance(value, str) and value in NET_ARCH_LIBRARY:
        return NET_ARCH_LIBRARY[value]
    if isinstance(value, list):
        return value
    return None


class PruningCallback(BaseCallback):
    """
    Optuna Pruning Callback:
    æ¯éš” eval_freq æ­¥æ•¸ï¼ŒåŸ·è¡Œä¸€æ¬¡è©•ä¼°ã€‚
    è‹¥è©•ä¼°çµæœä¸ä½³ï¼ˆç”± Optuna åˆ¤æ–·ï¼‰ï¼Œå‰‡æ‹‹å‡º TrialPruned ä¸­æ–·è¨“ç·´ã€‚
    """
    def __init__(
        self,
        trial: optuna.Trial,
        eval_env: VecEnv,
        eval_freq: int = 20000,
        n_eval_episodes: int = 3,
        metric: str = "portfolio",
        verbose: int = 0,
    ):
        super().__init__(verbose)
        self.trial = trial
        self.eval_env = eval_env
        self.eval_freq = eval_freq
        self.n_eval_episodes = n_eval_episodes
        self.metric = metric

    def _on_step(self) -> bool:
        if self.n_calls % self.eval_freq == 0:
            # åŸ·è¡Œè©•ä¼°
            score = evaluate_model(
                model=self.model,
                eval_env=self.eval_env,
                n_episodes=self.n_eval_episodes,
                metric=self.metric,
            )
            
            # å›å ±çµ¦ Optuna
            self.trial.report(score, self.n_calls)
            
            # æª¢æŸ¥æ˜¯å¦éœ€è¦å‰ªæ
            if self.trial.should_prune():
                raise TrialPruned()
                
        return True


def objective(trial: optuna.Trial, args: argparse.Namespace) -> float:
    """å–®ä¸€ trialï¼šå–æ¨£åƒæ•¸ -> è¨“ç·´ -> é©—è­‰ä¸¦å›å‚³ç¸¾æ•ˆã€‚"""

    hyperparams = suggest_hyperparams(trial)
    env_cfg = env_section_from_args(args)
    env_kwargs = build_env_kwargs(env_cfg, root_dir=ROOT)
    
    # å„ªåŒ–ï¼šä½¿ç”¨ SubprocVecEnv é€²è¡Œå¤šé€²ç¨‹ä¸¦è¡Œæ¡æ¨£
    # æ ¹æ“š CPU æ ¸å¿ƒæ•¸æ±ºå®šä¸¦è¡Œæ•¸é‡ï¼Œé€™è£¡ä¿å®ˆè¨­ç‚º 4
    n_envs = 4
    env_fns = [
        make_env(
            env_kwargs=env_kwargs,
            seed=trial.number * n_envs + i,
            random_start=args.random_start,
        )
        for i in range(n_envs)
    ]
    train_env = SubprocVecEnv(env_fns)

    # å„ªåŒ–ï¼šå»ºç«‹æŒä¹…åŒ–è©•ä¼°ç’°å¢ƒ (å¹³è¡ŒåŒ–)
    n_eval_envs = args.eval_episodes
    eval_kwargs = dict(env_kwargs)
    eval_kwargs["episode_length"] = args.eval_episode_length
    
    eval_env_fns = [
        make_env(
            env_kwargs=eval_kwargs,
            seed=trial.number * 200 + i,
            random_start=True
        )
        for i in range(n_eval_envs)
    ]
    eval_env = SubprocVecEnv(eval_env_fns)

    model = SAC(
        policy="MlpPolicy",
        env=train_env,
        learning_rate=hyperparams["learning_rate"],
        gamma=hyperparams["gamma"],
        batch_size=hyperparams["batch_size"],
        tau=hyperparams["tau"],
        buffer_size=args.buffer_size,
        train_freq=args.train_freq,
        gradient_steps=args.gradient_steps,
        policy_kwargs=hyperparams["policy_kwargs"],
        device=args.device,
        verbose=0,
    )

    # è¨­å®š Pruning Callback
    # é »ç‡è¨­ç‚ºç¸½æ­¥æ•¸çš„ 1/4ï¼Œå³è©•ä¼° 4 æ¬¡
    eval_freq = max(args.train_timesteps // 4, 1000)
    pruning_callback = PruningCallback(
        trial=trial,
        eval_env=eval_env,
        eval_freq=eval_freq,
        n_eval_episodes=n_eval_envs,
        metric=args.metric,
    )

    try:
        model.learn(total_timesteps=args.train_timesteps, callback=pruning_callback)
    except TrialPruned:
        train_env.close()
        eval_env.close()
        raise
    
    train_env.close()

    score = evaluate_model(
        model=model,
        eval_env=eval_env,
        n_episodes=n_eval_envs,
        metric=args.metric,
    )
    eval_env.close()
    return score


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(description="ä½¿ç”¨ Optuna æœå°‹ SAC è¶…åƒæ•¸")
    parser.add_argument("--config", type=Path, default=None, help="YAML/JSON è¨­å®šæª”ï¼Œç”¨æ–¼è¦†å¯« env/train åƒæ•¸")
    parser.add_argument("--csv_path", type=Path, default=DEFAULT_CSV)
    parser.add_argument("--episode_length", type=int, default=600, help="tuning è¨“ç·´æ™‚çš„ episode é•·åº¦")
    parser.add_argument("--fee_rate", type=float, default=0.0004)
    parser.add_argument("--lambda_inv", type=float, default=0.001)
    parser.add_argument("--lambda_turnover", type=float, default=0.0)
    parser.add_argument("--max_inventory", type=float, default=10.0)
    parser.add_argument("--base_spread", type=float, default=0.2)
    parser.add_argument("--alpha", type=float, default=1.0)
    parser.add_argument("--beta", type=float, default=0.5)
    parser.add_argument("--random_start", dest="random_start", action="store_true", help="é è¨­å¾éš¨æ©Ÿä½ç½®é–‹å§‹ episode")
    parser.add_argument("--fixed_start", dest="random_start", action="store_false", help="å›ºå®šå¾è³‡æ–™é–‹é ­é–‹å§‹ episode")
    parser.set_defaults(random_start=True)
    parser.add_argument("--train_timesteps", type=int, default=50_000)
    parser.add_argument("--buffer_size", type=int, default=50_000)
    parser.add_argument("--train_freq", type=int, default=1)
    parser.add_argument("--gradient_steps", type=int, default=1)
    parser.add_argument("--eval_episode_length", type=int, default=600)
    parser.add_argument("--eval_episodes", type=int, default=5)
    parser.add_argument("--metric", choices=["portfolio", "reward"], default="portfolio")
    parser.add_argument("--n_trials", type=int, default=30)
    parser.add_argument("--study_name", type=str, default="mm_sac_optuna")
    parser.add_argument("--storage", type=str, default=None, help="Optuna storageï¼Œä¾‹å¦‚ sqlite:///optuna.db")
    parser.add_argument("--save_best_params", action="store_true")
    parser.add_argument("--best_params_path", type=Path, default=BEST_PARAMS_PATH)
    parser.add_argument(
        "--device",
        type=str,
        default="auto",
        help="Stable-Baselines3 è£ç½®ï¼Œå¸¸è¦‹å€¼å¦‚ auto / cpu / cuda / mps",
    )
    return parser.parse_args()


def main() -> None:
    args = parse_args()
    apply_config_overrides(args)
    csv_path = Path(args.csv_path)
    if not csv_path.is_absolute():
        csv_path = ROOT / csv_path
    if not csv_path.exists():
        raise FileNotFoundError(f"æ‰¾ä¸åˆ°è³‡æ–™æª”æ¡ˆ {csv_path}ï¼Œè«‹å…ˆåŸ·è¡Œè³‡æ–™ä¸‹è¼‰è…³æœ¬ã€‚")
    args.csv_path = csv_path

    MODELS_DIR.mkdir(parents=True, exist_ok=True)

    study = optuna.create_study(
        direction="maximize",
        study_name=args.study_name,
        storage=args.storage,
        load_if_exists=args.storage is not None,
    )
    study.optimize(partial(objective, args=args), n_trials=args.n_trials)

    best_trial = study.best_trial
    print("=== Optuna æœ€ä½³çµæœ ===")
    print(f"score = {best_trial.value:.4f}")
    print("params =")
    for k, v in best_trial.params.items():
        readable = decode_net_arch(v) if k == "net_arch" else v
        print(f"  {k}: {readable}")

    if args.save_best_params:
        best_path = args.best_params_path
        best_path.parent.mkdir(parents=True, exist_ok=True)
        export_params = dict(best_trial.params)
        decoded_arch = decode_net_arch(export_params.get("net_arch"))
        if decoded_arch is not None:
            export_params["net_arch"] = decoded_arch
        with best_path.open("w", encoding="utf-8") as f:
            json.dump(export_params, f, indent=2, ensure_ascii=False)
        print(f"å·²å°‡æœ€ä½³è¶…åƒæ•¸å¯«å…¥ {best_path}")

    # æ–°å¢çµæŸæç¤º
    end_time = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    print(f"\n{'='*40}")
    print(f"âœ… Tuning Complete: {args.study_name}")
    print(f"ğŸ•’ End Time: {end_time}")
    print(f"{'='*40}\n")


if __name__ == "__main__":
    main()
